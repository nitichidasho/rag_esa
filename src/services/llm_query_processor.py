"""
LLMãƒ™ãƒ¼ã‚¹ã®é«˜åº¦ãªã‚¯ã‚¨ãƒªãƒ—ãƒ­ã‚»ãƒƒã‚µ
GPT/LLMã‚’ä½¿ç”¨ã—ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºã¨ã‚¯ã‚¨ãƒªæœ€é©åŒ–
"""

from typing import List, Dict, Any, Optional
import json
import asyncio
from dataclasses import dataclass
from loguru import logger

# å°†æ¥çš„ã«OpenAI APIã‚„ãƒ­ãƒ¼ã‚«ãƒ«LLMã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚ã®åŸºç›¤
# from openai import AsyncOpenAI
# from transformers import pipeline


@dataclass
class LLMQueryResult:
    """LLMå‡¦ç†çµæœ"""
    original_query: str
    extracted_keywords: List[str]
    search_intent: str  # "factual", "procedural", "comparative", etc.
    metadata_filters: Dict[str, Any]  # æ—¥ä»˜ã€ã‚«ãƒ†ã‚´ãƒªãªã©ã®ãƒ•ã‚£ãƒ«ã‚¿
    optimized_queries: List[str]  # è¤‡æ•°ã®æœ€é©åŒ–ã•ã‚ŒãŸã‚¯ã‚¨ãƒª
    confidence: float


class LLMQueryProcessor:
    """
    LLMã‚’ä½¿ç”¨ã—ãŸé«˜åº¦ãªã‚¯ã‚¨ãƒªå‡¦ç†
    
    æ©Ÿèƒ½:
    1. è‡ªç„¶è¨€èªã‹ã‚‰ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
    2. æ¤œç´¢æ„å›³ã®åˆ†é¡
    3. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿ã®æ¨å®š
    4. è¤‡æ•°ã‚¯ã‚¨ãƒªã®ç”Ÿæˆ
    """
    
    def __init__(self, model_type: str = "local"):
        self.model_type = model_type
        self.model = None
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        self.keyword_extraction_prompt = """
ã‚ãªãŸã¯æŠ€è¡“æ–‡æ›¸æ¤œç´¢ã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®è³ªå•ã‹ã‚‰ã€åŠ¹æœçš„ãªæ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

è³ªå•: {query}

ä»¥ä¸‹ã®å½¢å¼ã§JSONã‚’è¿”ã—ã¦ãã ã•ã„:
{{
    "keywords": ["ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰1", "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰2", ...],
    "technical_terms": ["æŠ€è¡“ç”¨èª1", "æŠ€è¡“ç”¨èª2", ...],
    "search_intent": "factual|procedural|troubleshooting|comparison",
    "topic_category": "ubuntu|python|ros|ai|general",
    "optimized_queries": ["æœ€é©åŒ–ã‚¯ã‚¨ãƒª1", "æœ€é©åŒ–ã‚¯ã‚¨ãƒª2", ...],
    "confidence": 0.8
}}

é‡è¦:
- ç ”ç©¶å®¤ã®æŠ€è¡“æ–‡æ›¸æ¤œç´¢ã«é©ã—ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
- Ubuntuã€Pythonã€ROSã€AI/æ©Ÿæ¢°å­¦ç¿’é–¢é€£ã®å°‚é–€ç”¨èªã‚’å„ªå…ˆ
- æ—¥æœ¬èªã¨è‹±èªã®ä¸¡æ–¹ã‚’è€ƒæ…®
- æ¤œç´¢ã«æœ‰åŠ¹ãª2-4å€‹ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«çµã‚‹
"""
        
        self._initialize_model()
    
    def _initialize_model(self):
        """ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–"""
        if self.model_type == "openai":
            # OpenAI GPTä½¿ç”¨ï¼ˆå°†æ¥å®Ÿè£…ï¼‰
            # self.client = AsyncOpenAI(api_key="your-api-key")
            logger.info("OpenAI model would be initialized here")
        elif self.model_type == "local":
            # ãƒ­ãƒ¼ã‚«ãƒ«LLMä½¿ç”¨ï¼ˆå°†æ¥å®Ÿè£…ï¼‰
            # self.model = pipeline("text-generation", model="elyza/ELYZA-japanese-Llama-2-7b-instruct")
            logger.info("Local LLM model would be initialized here")
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹
            logger.info("Using rule-based fallback")
    
    async def process_query_with_llm(self, query: str) -> LLMQueryResult:
        """
        LLMã‚’ä½¿ç”¨ã—ãŸã‚¯ã‚¨ãƒªå‡¦ç†ï¼ˆå°†æ¥å®Ÿè£…ï¼‰
        """
        if self.model_type == "openai":
            return await self._process_with_openai(query)
        elif self.model_type == "local":
            return await self._process_with_local_llm(query)
        else:
            return self._process_with_rules(query)
    
    async def _process_with_openai(self, query: str) -> LLMQueryResult:
        """OpenAI GPTã‚’ä½¿ç”¨ã—ãŸå‡¦ç†ï¼ˆå°†æ¥å®Ÿè£…ï¼‰"""
        # å®Ÿè£…ä¾‹ï¼ˆè¦APIè¨­å®šï¼‰:
        # response = await self.client.chat.completions.create(
        #     model="gpt-4-mini",
        #     messages=[
        #         {"role": "system", "content": "You are a technical search expert."},
        #         {"role": "user", "content": self.keyword_extraction_prompt.format(query=query)}
        #     ],
        #     temperature=0.3
        # )
        # 
        # result_json = json.loads(response.choices[0].message.content)
        # return LLMQueryResult(
        #     original_query=query,
        #     extracted_keywords=result_json["keywords"],
        #     search_intent=result_json["search_intent"],
        #     metadata_filters={"category": result_json["topic_category"]},
        #     optimized_queries=result_json["optimized_queries"],
        #     confidence=result_json["confidence"]
        # )
        
        logger.info("OpenAI processing not yet implemented")
        return self._process_with_rules(query)
    
    async def _process_with_local_llm(self, query: str) -> LLMQueryResult:
        """ãƒ­ãƒ¼ã‚«ãƒ«LLMã‚’ä½¿ç”¨ã—ãŸå‡¦ç†ï¼ˆå°†æ¥å®Ÿè£…ï¼‰"""
        # å®Ÿè£…ä¾‹ï¼ˆè¦ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ï¼‰:
        # prompt = self.keyword_extraction_prompt.format(query=query)
        # response = self.model(prompt, max_length=500, temperature=0.3)
        # result_text = response[0]['generated_text']
        # result_json = self._parse_llm_response(result_text)
        
        logger.info("Local LLM processing not yet implemented")
        return self._process_with_rules(query)
    
    def _process_with_rules(self, query: str) -> LLMQueryResult:
        """ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†"""
        # ç¾åœ¨ã®QueryProcessorã‚’æ´»ç”¨
        from ..utils.query_processor import QueryProcessor
        
        processor = QueryProcessor()
        result = processor.process_query(query)
        
        # æ¤œç´¢æ„å›³ã®æ¨å®š
        intent = self._classify_intent(query)
        
        # ãƒˆãƒ”ãƒƒã‚¯ã‚«ãƒ†ã‚´ãƒªã®æ¨å®š
        category = self._classify_category(query, result['keywords'])
        
        return LLMQueryResult(
            original_query=query,
            extracted_keywords=result['keywords'],
            search_intent=intent,
            metadata_filters={"category": category},
            optimized_queries=[result['recommended_query']],
            confidence=0.7  # ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã¯ä¸­ç¨‹åº¦ã®ä¿¡é ¼åº¦
        )
    
    def _classify_intent(self, query: str) -> str:
        """æ¤œç´¢æ„å›³ã®åˆ†é¡"""
        query_lower = query.lower()
        
        if any(word in query_lower for word in ['æ–¹æ³•', 'ã‚„ã‚Šæ–¹', 'how to', 'ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«', 'ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—']):
            return "procedural"
        elif any(word in query_lower for word in ['ã‚¨ãƒ©ãƒ¼', 'problem', 'å•é¡Œ', 'è§£æ±º', 'troubleshoot']):
            return "troubleshooting"
        elif any(word in query_lower for word in ['é•ã„', 'æ¯”è¼ƒ', 'compare', 'difference']):
            return "comparison"
        else:
            return "factual"
    
    def _classify_category(self, query: str, keywords: List[str]) -> str:
        """ãƒˆãƒ”ãƒƒã‚¯ã‚«ãƒ†ã‚´ãƒªã®åˆ†é¡"""
        query_text = (query + " " + " ".join(keywords)).lower()
        
        if any(term in query_text for term in ['ubuntu', 'linux', 'os']):
            return "ubuntu"
        elif any(term in query_text for term in ['python', 'ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°', 'ã‚³ãƒ¼ãƒ‰']):
            return "python"
        elif any(term in query_text for term in ['ros', 'robot', 'ãƒ­ãƒœãƒƒãƒˆ']):
            return "ros"
        elif any(term in query_text for term in ['ai', 'æ©Ÿæ¢°å­¦ç¿’', 'machine learning', 'deep learning']):
            return "ai"
        else:
            return "general"


class IterativeQueryProcessor:
    """
    IterKeyæ‰‹æ³•ã®å®Ÿè£…
    æ¤œç´¢çµæœã«åŸºã¥ã„ã¦ã‚¯ã‚¨ãƒªã‚’åå¾©æ”¹å–„
    """
    
    def __init__(self, max_iterations: int = 3):
        self.max_iterations = max_iterations
        self.llm_processor = LLMQueryProcessor()
    
    async def iterative_search(
        self, 
        initial_query: str,
        target_results: int = 5,
        quality_threshold: float = 1.5
    ) -> Dict[str, Any]:
        """
        åå¾©çš„ã‚¯ã‚¨ãƒªæ”¹å–„ã«ã‚ˆã‚‹æ¤œç´¢
        
        ãƒ—ãƒ­ã‚»ã‚¹:
        1. åˆæœŸã‚¯ã‚¨ãƒªã§æ¤œç´¢
        2. çµæœã®å“è³ªè©•ä¾¡
        3. ä¸ååˆ†ãªå ´åˆã€ã‚¯ã‚¨ãƒªã‚’æ”¹å–„ã—ã¦å†æ¤œç´¢
        4. æº€è¶³ã„ãçµæœãŒå¾—ã‚‰ã‚Œã‚‹ã¾ã§ç¹°ã‚Šè¿”ã—
        """
        search_history = []
        current_query = initial_query
        
        for iteration in range(self.max_iterations):
            logger.info(f"IterKey iteration {iteration + 1}: '{current_query}'")
            
            # LLMå‡¦ç†
            llm_result = await self.llm_processor.process_query_with_llm(current_query)
            
            # æ¤œç´¢å®Ÿè¡Œï¼ˆã“ã“ã§ã¯æ—¢å­˜ã®SearchServiceã‚’ä½¿ç”¨ï¼‰
            from .search_service import SearchService
            search_service = SearchService()
            
            search_results = search_service.semantic_search(
                llm_result.optimized_queries[0],
                limit=target_results * 2
            )
            
            # å“è³ªè©•ä¾¡
            high_quality_results = [r for r in search_results if r.score >= quality_threshold]
            
            iteration_result = {
                "iteration": iteration + 1,
                "query": current_query,
                "llm_processing": llm_result,
                "search_results": len(search_results),
                "high_quality_results": len(high_quality_results),
                "average_score": sum(r.score for r in search_results) / len(search_results) if search_results else 0
            }
            search_history.append(iteration_result)
            
            # çµ‚äº†æ¡ä»¶ãƒã‚§ãƒƒã‚¯
            if len(high_quality_results) >= target_results:
                logger.info(f"IterKey succeeded in {iteration + 1} iterations")
                break
            
            # æ¬¡ã®ã‚¯ã‚¨ãƒªç”Ÿæˆï¼ˆæ”¹å–„ãƒ­ã‚¸ãƒƒã‚¯ï¼‰
            if iteration < self.max_iterations - 1:
                current_query = self._generate_improved_query(
                    initial_query, 
                    llm_result, 
                    search_results
                )
        
        return {
            "final_query": current_query,
            "total_iterations": len(search_history),
            "search_history": search_history,
            "final_results": search_results[:target_results]
        }
    
    def _generate_improved_query(
        self, 
        original_query: str, 
        llm_result: LLMQueryResult, 
        previous_results: List[Any]
    ) -> str:
        """
        æ¤œç´¢çµæœã«åŸºã¥ã„ã¦ã‚¯ã‚¨ãƒªã‚’æ”¹å–„
        """
        # ç°¡å˜ãªæ”¹å–„ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆå®Ÿéš›ã«ã¯LLMã‚’ä½¿ç”¨ã—ã¦ã‚ˆã‚Šé«˜åº¦ã«ï¼‰
        if not previous_results:
            # çµæœãŒãªã„å ´åˆï¼šã‚ˆã‚Šä¸€èˆ¬çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è©¦ã™
            keywords = llm_result.extracted_keywords
            if len(keywords) > 1:
                return " ".join(keywords[:2])  # ã‚ˆã‚Šå°‘ãªã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§
        
        # çµæœãŒå°‘ãªã„å ´åˆï¼šåŒç¾©èªå±•é–‹
        if len(previous_results) < 3:
            return f"{llm_result.optimized_queries[0]} OR setup OR configuration"
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼šå…ƒã®ã‚¯ã‚¨ãƒªã‚’è¿”ã™
        return original_query


# å®Ÿè£…ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã¨TODO
"""
ğŸš€ å®Ÿè£…ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—

Phase 1 (å³åº§ã«å®Ÿè£…å¯èƒ½):
- ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®LLMQueryProcessor
- æ—¢å­˜QueryProcessorã¨ã®çµ±åˆ
- åŸºæœ¬çš„ãªIterativeQueryProcessor

Phase 2 (è¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒªè¿½åŠ ):
- ãƒ­ãƒ¼ã‚«ãƒ«LLMçµ±åˆï¼ˆtransformersä½¿ç”¨ï¼‰
- ã‚ˆã‚Šé«˜åº¦ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
- æ¤œç´¢çµæœã®è‡ªå‹•è©•ä¾¡

Phase 3 (è¦å¤–éƒ¨API):
- OpenAI GPTçµ±åˆ
- Claude/Geminiçµ±åˆ
- è¤‡æ•°LLMã®æ¯”è¼ƒè©•ä¾¡

å®Ÿè£…å„ªå…ˆåº¦:
1. HybridSearchService (å³åº§ã«åŠ¹æœã‚ã‚Š)
2. LLMQueryProcessor ã®ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹éƒ¨åˆ†
3. æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã¨ã®çµ±åˆ
4. IterativeQueryProcessor ã®åŸºæœ¬ç‰ˆ

"""
