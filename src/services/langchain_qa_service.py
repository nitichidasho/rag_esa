"""
LangChainãƒ™ãƒ¼ã‚¹ã®è³ªå•å¿œç­”ã‚µãƒ¼ãƒ“ã‚¹
Zennã‚µã‚¤ãƒˆã®å®Ÿè£…ã‚’å‚è€ƒã«ã—ãŸRAGã‚·ã‚¹ãƒ†ãƒ 
"""

import os
from datetime import datetime
from typing import List, Optional, Dict, Any
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch
from loguru import logger

from ..config.settings import settings
from ..models.qa import QAResult
from ..models.esa_models import Article
from .search_service import SearchService
from ..utils.query_processor import QueryProcessor


class LangChainQAService:
    """LangChainã‚¹ã‚¿ã‚¤ãƒ«ã®è³ªå•å¿œç­”ã‚µãƒ¼ãƒ“ã‚¹"""
    
    def __init__(self):
        # åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’è©¦è¡Œé †ã§å®šç¾©ï¼ˆQAç”¨é€”ã«æœ€é©åŒ–ï¼‰
        self.model_candidates = [
            "weblab-GENIAC/Tanuki-8B-dpo-v1.0",  # æ—¥æœ¬èªç‰¹åŒ–é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«
            "google/flan-t5-base",  # QAç”¨é€”ã«æœ€é©
            "google/flan-t5-large", # ã‚ˆã‚Šé«˜æ€§èƒ½
            "microsoft/GODEL-v1_1-large-seq2seq",  # å¯¾è©±å‹ã ãŒQAå¯èƒ½
            "facebook/blenderbot-400M-distill",  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            "microsoft/DialoGPT-large"  # æœ€å¾Œã®æ‰‹æ®µ
        ]
        
        self.model_name = None
        self.model = None
        self.tokenizer = None
        self.pipeline = None
        self.search_service = SearchService()
        self.query_processor = QueryProcessor()
        self._load_model()
    
    def _load_model(self):
        """åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ï¼ˆGPUå¯¾å¿œï¼‰"""
        # GPUç’°å¢ƒã®è©³ç´°æƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›
        self._log_device_info()
        
        for model_name in self.model_candidates:
            try:
                logger.info(f"Trying to load model: {model_name}")
                
                self.tokenizer = AutoTokenizer.from_pretrained(model_name)
                
                # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¨­å®š
                if self.tokenizer.pad_token is None:
                    self.tokenizer.pad_token = self.tokenizer.eos_token
                
                # GPUå¯¾å¿œã®ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
                device_config = self._get_optimal_device_config()
                
                # ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦ç•°ãªã‚‹èª­ã¿è¾¼ã¿æ–¹æ³•
                if "flan-t5" in model_name:
                    from transformers import T5ForConditionalGeneration
                    self.model = T5ForConditionalGeneration.from_pretrained(
                        model_name,
                        torch_dtype=device_config["dtype"],
                        device_map=device_config["device_map"]
                    )
                    self.pipeline = pipeline(
                        "text2text-generation",
                        model=self.model,
                        tokenizer=self.tokenizer,
                        torch_dtype=torch.float16 if device_config["device_type"].startswith("GPU") else torch.float32,
                        device_map="auto",  # accelerateã«ä»»ã›ã‚‹
                        max_length=500,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®æœ€å¤§é•·ã‚’å¤§å¹…ã«å¢—åŠ 
                        do_sample=True,
                        temperature=0.7
                    )
                else:
                    self.model = AutoModelForCausalLM.from_pretrained(
                        model_name,
                        torch_dtype=device_config["dtype"],
                        device_map=device_config["device_map"],
                        low_cpu_mem_usage=True  # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–
                    )
                    # accelerateä½¿ç”¨æ™‚ã¯deviceå¼•æ•°ã‚’å‰Šé™¤
                    self.pipeline = pipeline(
                        "text-generation",
                        model=self.model,
                        tokenizer=self.tokenizer,
                        torch_dtype=torch.float16 if device_config["device_type"].startswith("GPU") else torch.float32,
                        device_map="auto",  # accelerateã«ä»»ã›ã‚‹
                        max_length=512,
                        do_sample=True,
                        temperature=0.7,
                        return_full_text=False
                    )
                
                self.model_name = model_name
                self.device_info = device_config
                logger.info(f"Successfully loaded model: {model_name} on {device_config['device_type']}")
                break
                
            except Exception as e:
                logger.warning(f"Failed to load model {model_name}: {e}")
                continue
        
    def _log_device_info(self):
        """ãƒ‡ãƒã‚¤ã‚¹æƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›"""
        logger.info(f"PyTorch version: {torch.__version__}")
        logger.info(f"CUDA available: {torch.cuda.is_available()}")
        
        if torch.cuda.is_available():
            logger.info(f"CUDA device count: {torch.cuda.device_count()}")
            for i in range(torch.cuda.device_count()):
                device_name = torch.cuda.get_device_name(i)
                memory_total = torch.cuda.get_device_properties(i).total_memory / 1024**3
                logger.info(f"GPU {i}: {device_name} ({memory_total:.1f}GB)")
        else:
            logger.info("Running on CPU")
    
    def _get_optimal_device_config(self):
        """æœ€é©ãªãƒ‡ãƒã‚¤ã‚¹è¨­å®šã‚’è¿”ã™"""
        if torch.cuda.is_available():
            # GPUåˆ©ç”¨å¯èƒ½ãªå ´åˆ
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            
            if gpu_memory >= 12:  # 12GBä»¥ä¸Šã®GPU
                return {
                    "device_type": "GPU (High Memory)",
                    "device_map": "auto",
                    "dtype": torch.float16,
                    "pipeline_device": 0,
                    "batch_size": 8
                }
            elif gpu_memory >= 6:  # 6GBä»¥ä¸Šã®GPU
                return {
                    "device_type": "GPU (Medium Memory)",
                    "device_map": "auto",
                    "dtype": torch.float16,
                    "pipeline_device": 0,
                    "batch_size": 4
                }
            else:  # 6GBæœªæº€ã®GPU
                return {
                    "device_type": "GPU (Low Memory)",
                    "device_map": "auto",
                    "dtype": torch.float16,
                    "pipeline_device": 0,
                    "batch_size": 2
                }
        else:
            # CPUåˆ©ç”¨ã®å ´åˆ
            return {
                "device_type": "CPU",
                "device_map": None,
                "dtype": torch.float32,
                "pipeline_device": -1,
                "batch_size": 1
            }
    
    def answer_question(self, question: str, context_limit: int = 5) -> QAResult:
        """è³ªå•ã«å¯¾ã™ã‚‹å›ç­”ã‚’ç”Ÿæˆï¼ˆLangChainã‚¹ã‚¿ã‚¤ãƒ«ï¼‰"""
        try:
            logger.info(f"Processing question: {question[:50]}...")
            
            # Step 0: ã‚¯ã‚¨ãƒªå‰å‡¦ç† - è‡ªç„¶è¨€èªè³ªå•ã‹ã‚‰åŠ¹æœçš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
            query_result = self.query_processor.process_query(question)
            optimized_query = query_result['recommended_query']
            
            logger.info(f"ã‚¯ã‚¨ãƒªå‰å‡¦ç†çµæœ:")
            logger.info(f"  å…ƒã®è³ªå•: {question}")
            logger.info(f"  æœ€é©åŒ–ã‚¯ã‚¨ãƒª: {optimized_query}")
            logger.info(f"  æŠ½å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {query_result['keywords']}")
            logger.info(f"  æŠ€è¡“ç”¨èª: {query_result['technical_terms']}")
            
            # Step 1: æ¤œç´¢ãƒ•ã‚§ãƒ¼ã‚º - æœ€é©åŒ–ã•ã‚ŒãŸã‚¯ã‚¨ãƒªã§æ¤œç´¢
            extended_limit = max(context_limit * 2, 10)
            logger.info(f"LangChainæ¤œç´¢å®Ÿè¡Œ: æœ€é©åŒ–ã‚¯ã‚¨ãƒª='{optimized_query}', å–å¾—ä»¶æ•°={extended_limit}")
            
            # ã¾ãšæœ€é©åŒ–ã‚¯ã‚¨ãƒªã§æ¤œç´¢
            search_results = self.search_service.semantic_search(
                query=optimized_query,
                limit=extended_limit,
                debug_mode=True
            )
            
            logger.info(f"æœ€é©åŒ–ã‚¯ã‚¨ãƒªæ¤œç´¢çµæœ: {len(search_results)}ä»¶")
            
            # æœ€é©åŒ–ã‚¯ã‚¨ãƒªã§çµæœãŒå°‘ãªã„å ´åˆã€å…ƒã®è³ªå•ã§ã‚‚æ¤œç´¢
            if len(search_results) < 3 and optimized_query != question:
                logger.info(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢å®Ÿè¡Œ: å…ƒã®è³ªå•='{question}'")
                fallback_results = self.search_service.semantic_search(
                    query=question,
                    limit=extended_limit,
                    debug_mode=True
                )
                
                # çµæœã‚’ãƒãƒ¼ã‚¸ï¼ˆé‡è¤‡é™¤å»ï¼‰
                seen_articles = set()
                merged_results = []
                
                # æœ€é©åŒ–ã‚¯ã‚¨ãƒªã®çµæœã‚’å„ªå…ˆ
                for result in search_results:
                    if result.article.number not in seen_articles:
                        merged_results.append(result)
                        seen_articles.add(result.article.number)
                
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯çµæœã‚’è¿½åŠ 
                for result in fallback_results:
                    if result.article.number not in seen_articles and len(merged_results) < extended_limit:
                        merged_results.append(result)
                        seen_articles.add(result.article.number)
                
                search_results = merged_results
                logger.info(f"ãƒãƒ¼ã‚¸å¾Œæ¤œç´¢çµæœ: {len(search_results)}ä»¶")
            
            logger.info(f"LangChainæ¤œç´¢çµæœ: {len(search_results)}ä»¶ã®è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
            
            # æ¤œç´¢çµæœã®å“è³ªãƒã‚§ãƒƒã‚¯ï¼ˆQAServiceã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯ï¼‰
            high_quality_results = []
            for result in search_results:
                # ã‚¿ã‚¤ãƒˆãƒ«ãƒãƒƒãƒã¾ãŸã¯é«˜ã‚¹ã‚³ã‚¢ã®è¨˜äº‹ã‚’å„ªå…ˆ
                if result.score >= 1.5:  # ã‚¿ã‚¤ãƒˆãƒ«ãƒãƒƒãƒï¼ˆã‚¹ã‚³ã‚¢2.0ï¼‰ã‚„é«˜å“è³ªãªçµæœ
                    high_quality_results.append(result)
                    logger.info(f"LangChainé«˜å“è³ªçµæœ: {result.article.name} (ã‚¹ã‚³ã‚¢: {result.score:.3f})")
            
            # æœ€çµ‚çš„ã«ä½¿ç”¨ã™ã‚‹è¨˜äº‹ã‚’æ±ºå®š
            final_results = high_quality_results[:context_limit] if high_quality_results else search_results[:context_limit]
            
            if not final_results:
                logger.warning(f"LangChainæ¤œç´¢ã§é–¢é€£è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ: '{question}'")
                return QAResult(
                    question=question,
                    answer="ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€ã”è³ªå•ã®å†…å®¹ã¯å·åˆç ”ç©¶å®¤ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«å«ã¾ã‚Œã¦ã„ãªã„ã‚ˆã†ã§ã™ã€‚\n\nã“ã®ã‚·ã‚¹ãƒ†ãƒ ã¯å·åˆç ”ç©¶å®¤ã®ç ”ç©¶æ´»å‹•ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã€æŠ€è¡“çš„ãªå–ã‚Šçµ„ã¿ã«é–¢ã™ã‚‹æƒ…å ±ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã™ã€‚ ä¸€èˆ¬çš„ãªæŠ€è¡“æƒ…å ±ã«ã¤ã„ã¦ã¯ã€å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚„ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚¬ã‚¤ãƒ‰ã‚’ã”å‚ç…§ãã ã•ã„ã€‚\n\nå·åˆç ”ç©¶å®¤ã«é–¢ã™ã‚‹ã”è³ªå•ã§ã—ãŸã‚‰ã€ãŠæ°—è»½ã«ãŠèã‹ã›ãã ã•ã„ã€‚",
                    source_articles=[],
                    confidence=0.0,
                    generated_at=datetime.now()
                )
            
            logger.info(f"LangChainå›ç­”ç”Ÿæˆç”¨è¨˜äº‹: {len(final_results)}ä»¶ã‚’ä½¿ç”¨")
            for i, result in enumerate(final_results):
                logger.info(f"  {i+1}. {result.article.name} (ã‚¹ã‚³ã‚¢: {result.score:.3f})")
            
            # Step 2: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰
            context = self._build_context(final_results)
            logger.info(f"Built context with {len(final_results)} articles")
            
            # Step 3: ç”Ÿæˆãƒ•ã‚§ãƒ¼ã‚º - ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­è¨ˆã¨LLMå‘¼ã³å‡ºã—
            answer = self._generate_answer_with_prompt(question, context)
            
            # Step 4: ä¿¡é ¼åº¦è¨ˆç®—
            confidence = self._calculate_confidence(final_results)
            
            logger.info(f"Generated answer with confidence: {confidence:.2f}")
            
            return QAResult(
                question=question,
                answer=answer,
                source_articles=[result.article for result in final_results],
                confidence=confidence,
                generated_at=datetime.now()
            )
            
        except Exception as e:
            logger.error(f"Failed to answer question: {e}")
            return QAResult(
                question=question,
                answer="ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã—ã°ã‚‰ãå¾Œã«ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚",
                source_articles=[],
                confidence=0.0,
                generated_at=datetime.now()
            )
    
    def _build_context(self, search_results: List) -> str:
        """æ¤œç´¢çµæœã‹ã‚‰ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹ç¯‰ï¼ˆæ”¹å–„ç‰ˆï¼‰"""
        context_parts = []
        
        for i, result in enumerate(search_results, 1):
            article = result.article
            
            # è¨˜äº‹ã®åŸºæœ¬æƒ…å ±
            context_part = f"### è¨˜äº‹{i}\n"
            context_part += f"ã‚¿ã‚¤ãƒˆãƒ«: {article.name}\n"
            
            if article.category:
                context_part += f"åˆ†é‡: {article.category}\n"
            
            if article.tags:
                relevant_tags = [tag for tag in article.tags if tag.strip()][:3]  # æœ€å¤§3ã¤ã®ã‚¿ã‚°
                if relevant_tags:
                    context_part += f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(relevant_tags)}\n"
            
            # ã‚ˆã‚Šé–¢é€£æ€§ã®é«˜ã„ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹ã‚’ä½¿ç”¨
            content = ""
            if hasattr(result, 'matched_text') and result.matched_text:
                content = result.matched_text
            elif hasattr(article, 'processed_text') and article.processed_text:
                # é‡è¦ãªæ–‡ã‚’æŠ½å‡º
                content = self._extract_key_sentences(article.processed_text, 3)
            elif article.body_md:
                # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³è¨˜å·ã‚’é™¤å»ã—ã¦é‡è¦éƒ¨åˆ†ã‚’æŠ½å‡º
                clean_text = self._clean_markdown(article.body_md)
                content = self._extract_key_sentences(clean_text, 3)
            
            if content:
                context_part += f"å†…å®¹:\n{content}\n"
            
            context_part += f"é–¢é€£åº¦: {result.score:.2f}\n\n"
            context_parts.append(context_part)
        
        return "".join(context_parts)
    
    def _extract_key_sentences(self, text: str, num_sentences: int = 3) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰é‡è¦ãªæ–‡ã‚’æŠ½å‡º"""
        try:
            sentences = [s.strip() for s in text.split('ã€‚') if len(s.strip()) > 10]
            
            if len(sentences) <= num_sentences:
                return 'ã€‚'.join(sentences) + 'ã€‚'
            
            # é•·ã•ã¨æƒ…å ±é‡ã§ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
            scored_sentences = []
            for sentence in sentences:
                score = len(sentence)  # åŸºæœ¬ã‚¹ã‚³ã‚¢ï¼šæ–‡ã®é•·ã•
                
                # ç ”ç©¶å®¤é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒœãƒ¼ãƒŠã‚¹
                research_keywords = ['ç ”ç©¶', 'é–‹ç™º', 'æŠ€è¡“', 'å­¦ç¿’', 'å®Ÿé¨“', 'åˆ†æ', 'æˆæœ', 'AI', 'æ©Ÿæ¢°å­¦ç¿’']
                for keyword in research_keywords:
                    if keyword in sentence:
                        score += 50
                
                scored_sentences.append((sentence, score))
            
            # ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆã—ã¦ä¸Šä½ã‚’é¸æŠ
            scored_sentences.sort(key=lambda x: x[1], reverse=True)
            selected = [s[0] for s in scored_sentences[:num_sentences]]
            
            return 'ã€‚'.join(selected) + 'ã€‚'
            
        except Exception:
            return text[:300] + "..." if len(text) > 300 else text
    
    def _clean_markdown(self, text: str) -> str:
        """ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³è¨˜å·ã‚’é™¤å»ã—ã¦ã‚¯ãƒªãƒ¼ãƒ³ãªãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ"""
        import re
        
        # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³è¨˜å·ã‚’é™¤å»
        text = re.sub(r'#{1,6}\s*', '', text)  # ãƒ˜ãƒƒãƒ€ãƒ¼
        text = re.sub(r'\*{1,2}([^*]+)\*{1,2}', r'\1', text)  # å¼·èª¿
        text = re.sub(r'`([^`]+)`', r'\1', text)  # ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³ã‚³ãƒ¼ãƒ‰
        text = re.sub(r'```[\s\S]*?```', '', text)  # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯
        text = re.sub(r'\[([^\]]+)\]\([^)]+\)', r'\1', text)  # ãƒªãƒ³ã‚¯
        text = re.sub(r'!\[([^\]]*)\]\([^)]+\)', '', text)  # ç”»åƒ
        text = re.sub(r'\n{3,}', '\n\n', text)  # ä½™åˆ†ãªæ”¹è¡Œ
        
        return text.strip()
    
    def _generate_answer_with_prompt(self, question: str, context: str) -> str:
        """Zennã‚µã‚¤ãƒˆã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­è¨ˆã‚’å‚è€ƒã«ã—ãŸå›ç­”ç”Ÿæˆ"""
        
        # Zennã‚µã‚¤ãƒˆã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’å‚è€ƒ
        template = """ã‚ãªãŸã¯ä¸ãˆã‚‰ã‚ŒãŸæ–‡æ›¸ã®å†…å®¹ã«åŸºã¥ã„ã¦è³ªå•ã«ç­”ãˆã‚‹å°‚é–€å®¶ã§ã™ã€‚
è³ªå•ã«å¯¾ã—ã¦ã€æ–‡æ›¸ã®æƒ…å ±ã®ã¿ã‚’ä½¿ç”¨ã—ã¦è©³ç´°ã‹ã¤æ­£ç¢ºã«å›ç­”ã—ã¦ãã ã•ã„ã€‚
æ–‡æ›¸ã«é–¢é€£ã™ã‚‹æƒ…å ±ãŒãªã„å ´åˆã¯ã€ã€Œç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€ã“ã®æ–‡æ›¸ã«ã¯ãã®æƒ…å ±ãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€ã¨ç­”ãˆã¦ãã ã•ã„ã€‚

æ–‡è„ˆæƒ…å ±:
{context}

è³ªå•: {question}

å›ç­”:"""
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰
        prompt = template.format(context=context, question=question)
        
        try:
            # ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸç”Ÿæˆ
            if "flan-t5" in self.model_name:
                # T5ã®å ´åˆ - è‡ªç„¶ãªä¼šè©±å½¢å¼ã®æ—¥æœ¬èªå›ç­”ã‚’ä¿ƒé€²
                try:
                    # è³ªå•ã®ç¨®é¡ã‚’åˆ¤å®š
                    question_type = self._analyze_question_type(question)
                    
                    # ã‚ˆã‚Šå˜ç´”ã§åŠ¹æœçš„ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
                    simple_prompt = f"""ä»¥ä¸‹ã®æƒ…å ±ã‚’å‚è€ƒã«ã€è³ªå•ã«ä¸å¯§ãªæ—¥æœ¬èªã§è©³ã—ãç­”ãˆã¦ãã ã•ã„ã€‚

å‚è€ƒæƒ…å ±ï¼š
{context[:800]}

è³ªå•ï¼š{question}
å›ç­”ï¼š"""
                    
                    response = self.pipeline(
                        simple_prompt,
                        max_length=400,  # ååˆ†ãªé•·ã•ã‚’ç¢ºä¿ã—ã¦å®Œå…¨ãªå›ç­”ã‚’ç”Ÿæˆ
                        min_length=30,   # æœ€å°é•·ã‚’å¢—ã‚„ã—ã¦æ„å‘³ã®ã‚ã‚‹å›ç­”ã‚’ç¢ºä¿
                        num_beams=2,     # ãƒ“ãƒ¼ãƒ æ•°ã‚’å¢—ã‚„ã—ã¦å“è³ªå‘ä¸Š
                        no_repeat_ngram_size=3,  # ç¹°ã‚Šè¿”ã—é˜²æ­¢ã‚’é©åº¦ã«è¨­å®š
                        do_sample=False,  # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’ç„¡åŠ¹åŒ–ã—ã¦å®‰å®šæ€§å‘ä¸Š
                        early_stopping=True,
                        repetition_penalty=1.2,  # ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’é©åº¦ã«èª¿æ•´
                        length_penalty=1.0,  # é•·ã•ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’è¿½åŠ 
                        pad_token_id=self.tokenizer.pad_token_id,
                        eos_token_id=self.tokenizer.eos_token_id
                    )
                    if response and len(response) > 0:
                        raw_answer = response[0]['generated_text']
                        logger.info(f"T5 raw output length: {len(raw_answer)}")
                        logger.info(f"T5 raw output: '{raw_answer}'")  # å…¨æ–‡ã‚’ãƒ­ã‚°å‡ºåŠ›
                        
                        # å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’é™¤å»
                        if "å›ç­”:" in raw_answer:
                            answer = raw_answer.split("å›ç­”:")[-1].strip()
                            logger.info(f"Answer after 'å›ç­”:' split: '{answer}'")
                        elif "ç­”ãˆ:" in raw_answer:
                            answer = raw_answer.split("ç­”ãˆ:")[-1].strip()
                            logger.info(f"Answer after 'ç­”ãˆ:' split: '{answer}'")
                        else:
                            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå…¨ä½“ã‚’é™¤å»
                            if len(raw_answer) > len(simple_prompt):
                                answer = raw_answer[len(simple_prompt):].strip()
                                logger.info(f"Answer after prompt removal: '{answer}'")
                            else:
                                answer = raw_answer.strip()
                                logger.info(f"Answer as-is (shorter than prompt): '{answer}'")
                        
                        logger.info(f"T5 processed answer length: {len(answer)}")
                        logger.info(f"T5 processed answer: '{answer}'")
                        
                        # è‡ªç„¶ãªä¼šè©±å½¢å¼ã«èª¿æ•´
                        answer = self._make_conversational(answer, question_type)
                        logger.info(f"T5 conversational answer: '{answer}'")
                    else:
                        answer = "å›ç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
                        logger.warning("T5 generated empty response")
                except Exception as t5_error:
                    logger.error(f"T5 generation error: {t5_error}")
                    answer = self._create_fallback_answer(question, context)
            else:
                # DialoGPTãªã©ã®ç”Ÿæˆç³»ãƒ¢ãƒ‡ãƒ«ã®å ´åˆ
                try:
                    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’çŸ­ç¸®ã—ã¦ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™å†…ã«åã‚ã‚‹
                    short_prompt = self._create_short_prompt(question, context)
                    logger.info(f"Generation prompt: {short_prompt[:200]}...")
                    
                    response = self.pipeline(
                        short_prompt,
                        max_new_tokens=200,  # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’å¢—ã‚„ã—ã¦ååˆ†ãªå›ç­”ã‚’ç”Ÿæˆ
                        do_sample=True,
                        temperature=0.7,  # æ¸©åº¦ã‚’é©åº¦ã«è¨­å®š
                        top_p=0.9,
                        repetition_penalty=1.2,
                        pad_token_id=self.tokenizer.eos_token_id,
                        eos_token_id=self.tokenizer.eos_token_id,
                        return_full_text=False  # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å«ã¾ãªã„
                    )
                    
                    logger.info(f"Pipeline response: {response}")
                    
                    if response and len(response) > 0:
                        generated_text = response[0]['generated_text']
                        logger.info(f"Generated text: '{generated_text}'")
                        
                        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã‚’é™¤å»
                        if "å›ç­”:" in generated_text:
                            answer = generated_text.split("å›ç­”:")[-1].strip()
                        elif "ç­”ãˆ:" in generated_text:
                            answer = generated_text.split("ç­”ãˆ:")[-1].strip()
                        else:
                            answer = generated_text.strip()
                        
                        logger.info(f"Processed answer: '{answer}'")
                    else:
                        logger.warning("Pipeline returned empty response")
                        answer = "å›ç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
                        
                except Exception as gen_error:
                    logger.error(f"Generation error: {gen_error}")
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šã‚·ãƒ³ãƒ—ãƒ«ãªè¦ç´„ã‚’ä½œæˆ
                    answer = self._create_fallback_answer(question, context)
            
            # å¾Œå‡¦ç†ï¼šä½™åˆ†ãªæ–‡å­—ã‚’é™¤å»
            answer = self._post_process_answer(answer)
            logger.info(f"Final processed answer: '{answer[:100]}...'")
            
            # ä¸é©åˆ‡ãªå›ç­”ã®å ´åˆã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã«åˆ‡ã‚Šæ›¿ãˆï¼ˆæ¡ä»¶ã‚’å³æ ¼åŒ–ï¼‰
            if ("é©åˆ‡ãªå›ç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸ" in answer and "ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯" in answer):
                logger.info("Using fallback answer due to processing failure")
                answer = self._create_fallback_answer(question, context)
            
            return answer if answer else self._create_fallback_answer(question, context)
            
        except Exception as e:
            logger.error(f"Failed to generate answer: {e}")
            return self._create_fallback_answer(question, context)
    
    def _post_process_answer(self, answer: str) -> str:
        """å›ç­”ã®å¾Œå‡¦ç†"""
        original_answer = answer
        logger.info(f"Post-processing answer: '{answer}'")  # å…¨æ–‡ã‚’ãƒ­ã‚°å‡ºåŠ›
        
        # æ”¹è¡Œã‚’æ•´ç†
        answer = answer.replace('\n\n', '\n').strip()
        
        # ç‰¹æ®Šæ–‡å­—ã®å‡¦ç†
        answer = answer.replace('</s>', '').replace('<s>', '')
        answer = answer.replace('<pad>', '').replace('<unk>', '')
        
        # DialoGPTç‰¹æœ‰ã®å•é¡Œæ–‡å­—åˆ—ã‚’é™¤å»
        problematic_phrases = ['ILY', 'I love you', 'lol', 'haha', ':)', ':(', 'XD']
        for phrase in problematic_phrases:
            answer = answer.replace(phrase, '')
        
        # æ˜ã‚‰ã‹ã«å•é¡Œã®ã‚ã‚‹çµµæ–‡å­—ã®ã¿æ¤œå‡ºï¼ˆæ¡ä»¶ã‚’å¤§å¹…ã«ç·©å’Œï¼‰
        if 'ğŸ˜€' in answer or 'ğŸ˜‚' in answer or ':smiling_face' in answer.lower():
            logger.warning(f"Detected obvious problematic emojis: '{answer}'")
            return "é©åˆ‡ãªå›ç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å›ç­”ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚"
        
        # ç©ºã®å›ç­”ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆæœ€å„ªå…ˆï¼‰
        if not answer or len(answer.strip()) < 3:
            logger.warning(f"Empty or too short answer: '{answer}'")
            return "é©åˆ‡ãªå›ç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
        
        # çŸ­ã™ãã‚‹å›ç­”ã®åŸºæº–ã‚’ã•ã‚‰ã«ç·©å’Œï¼ˆ15æ–‡å­—ä»¥ä¸Šãªã‚‰è¨±å¯ï¼‰
        if len(answer.strip()) < 15:
            logger.warning(f"Answer too short: '{answer}' (length: {len(answer.strip())})")
            return "é©åˆ‡ãªå›ç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å›ç­”ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚"
        
        # æ„å‘³ã®ãªã„å›ç­”ã‚’ãƒã‚§ãƒƒã‚¯
        if answer.strip().lower() in ['yes', 'no', 'ok', 'sure']:
            logger.warning(f"Meaningless answer: '{answer}'")
            return "é©åˆ‡ãªå›ç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å›ç­”ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚"
        
        # æ•°å­—ã®ã¿ã®å›ç­”ã‚’æ¤œå‡º
        if answer.strip().replace(' ', '').replace(':', '').replace('###', '').isdigit():
            logger.warning(f"Detected numeric-only output: '{answer}'")
            return "é©åˆ‡ãªå›ç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å›ç­”ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚"
        
        # URLã®ã¿ã®å›ç­”ã‚’æ¤œå‡º
        if answer.strip().startswith('http') and ' ' not in answer.strip():
            logger.warning(f"URL-only answer detected: '{answer}'")
            return "é©åˆ‡ãªå›ç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å›ç­”ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚"
        
        logger.info(f"Answer passed post-processing: '{answer}'")
        return answer.strip()
    
    def _analyze_question_type(self, question: str) -> str:
        """è³ªå•ã®ã‚¿ã‚¤ãƒ—ã‚’åˆ†æ"""
        question_lower = question.lower()
        
        if any(word in question_lower for word in ['ã©ã®ã‚ˆã†ãª', 'ã©ã‚“ãª', 'ãªã‚“ã®', 'ä½•ã®']):
            return 'descriptive'  # èª¬æ˜çš„ãªè³ªå•
        elif any(word in question_lower for word in ['ã„ã¤', 'when', 'æ™‚æœŸ']):
            return 'temporal'     # æ™‚é–“é–¢é€£ã®è³ªå•
        elif any(word in question_lower for word in ['ã©ã“', 'where', 'å ´æ‰€']):
            return 'location'     # å ´æ‰€é–¢é€£ã®è³ªå•
        elif any(word in question_lower for word in ['ãªãœ', 'why', 'ç†ç”±']):
            return 'causal'       # åŸå› ãƒ»ç†ç”±ã®è³ªå•
        elif any(word in question_lower for word in ['æ•™ãˆã¦', 'çŸ¥ã‚ŠãŸã„', 'å‹•å‘', 'çŠ¶æ³']):
            return 'informational'  # æƒ…å ±å–å¾—ã®è³ªå•
        else:
            return 'general'      # ä¸€èˆ¬çš„ãªè³ªå•
    
    def _create_conversational_prompt(self, question: str, context: str, question_type: str) -> str:
        """è³ªå•ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸè‡ªç„¶ãªä¼šè©±å½¢å¼ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ"""
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’é©åˆ‡ãªé•·ã•ã«èª¿æ•´
        trimmed_context = context[:1000]
        
        if question_type == 'informational':
            return f"""ã‚ãªãŸã¯ç ”ç©¶å®¤ã®æ´»å‹•ã«ã¤ã„ã¦è©³ã—ã„å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®æƒ…å ±ã‚’åŸºã«ã€è³ªå•ã«å¯¾ã—ã¦ä¸å¯§ã§åˆ†ã‹ã‚Šã‚„ã™ã„æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚

ç ”ç©¶å®¤æƒ…å ±:
{trimmed_context}

è³ªå•: {question}

å›ç­”: å·åˆç ”ç©¶å®¤ã®å‹•å‘ã«ã¤ã„ã¦ãŠç­”ãˆã—ã¾ã™ã€‚"""
        
        elif question_type == 'descriptive':
            return f"""ä»¥ä¸‹ã®æƒ…å ±ã‚’å‚è€ƒã«ã€è³ªå•ã«ã¤ã„ã¦å…·ä½“çš„ã§è©³ç´°ãªèª¬æ˜ã‚’ã—ã¦ãã ã•ã„ã€‚

æƒ…å ±:
{trimmed_context}

è³ªå•: {question}

å›ç­”:"""
        
        else:
            return f"""ä»¥ä¸‹ã®ç ”ç©¶å®¤ã«é–¢ã™ã‚‹æƒ…å ±ã‚’åŸºã«ã€è³ªå•ã«å¯¾ã—ã¦è‡ªç„¶ã§åˆ†ã‹ã‚Šã‚„ã™ã„æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚

æƒ…å ±:
{trimmed_context}

è³ªå•: {question}

å›ç­”:"""
    
    def _make_conversational(self, answer: str, question_type: str) -> str:
        """å›ç­”ã‚’ã‚ˆã‚Šè‡ªç„¶ãªä¼šè©±å½¢å¼ã«èª¿æ•´"""
        if not answer or len(answer.strip()) < 10:
            return answer
        
        # æ—¢ã«é©åˆ‡ãªå›ç­”ã®å ´åˆã¯ãã®ã¾ã¾è¿”ã™
        if answer.startswith(('å·åˆç ”ç©¶å®¤', 'ã“ã¡ã‚‰', 'ã“ã®', 'ç ”ç©¶å®¤')):
            return answer
        
        # è³ªå•ã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦è‡ªç„¶ãªå°å…¥éƒ¨ã‚’è¿½åŠ 
        if question_type == 'informational':
            if not answer.startswith(('å·åˆç ”ç©¶å®¤', 'æ¤œç´¢çµæœ')):
                answer = f"å·åˆç ”ç©¶å®¤ã®å‹•å‘ã«ã¤ã„ã¦ã€{answer}"
        
        # æ–‡æœ«ã®èª¿æ•´
        if not answer.endswith(('.', 'ã€‚', 'ã™', 'ã¾ã™', 'ã—ãŸ', 'ã§ã—ãŸ')):
            if 'ã§ã™' not in answer and 'ã¾ã™' not in answer:
                answer += "ã€‚"
        
        return answer
    
    def _create_short_prompt(self, question: str, context: str) -> str:
        """çŸ­ç¸®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆï¼ˆãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™å¯¾å¿œï¼‰"""
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’é©åˆ‡ãªé•·ã•ã«èª¿æ•´
        short_context = context[:600]  # Tanuki-8Bç”¨ã«å°‘ã—é•·ã‚ã«è¨­å®š
        
        # ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        if "flan-t5" in self.model_name:
            # T5ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            short_prompt = f"""ä»¥ä¸‹ã®æ–‡æ›¸ã«åŸºã¥ã„ã¦è³ªå•ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚

æ–‡æ›¸: {short_context}

è³ªå•: {question}"""
        elif "Tanuki" in self.model_name:
            # Tanuki-8Bç”¨ã®æœ€é©åŒ–ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            short_prompt = f"""ä»¥ä¸‹ã¯å·åˆç ”ç©¶å®¤ã«é–¢ã™ã‚‹æ–‡æ›¸ã§ã™ã€‚ã“ã®æƒ…å ±ã‚’å‚è€ƒã«ã—ã¦ã€è³ªå•ã«è©³ã—ãå›ç­”ã—ã¦ãã ã•ã„ã€‚

ã€å‚è€ƒæ–‡æ›¸ã€‘
{short_context}

ã€è³ªå•ã€‘
{question}

ã€å›ç­”ã€‘
"""
        else:
            # ãã®ä»–ã®ç”Ÿæˆç³»ãƒ¢ãƒ‡ãƒ«ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            short_prompt = f"""æ–‡æ›¸ã‚’å‚è€ƒã«è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚

æ–‡æ›¸: {short_context}

è³ªå•: {question}
å›ç­”:"""
        return short_prompt
    
    def _create_fallback_answer(self, question: str, context: str) -> str:
        """æ”¹å–„ã•ã‚ŒãŸãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å›ç­”ã‚’ä½œæˆ"""
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰é‡è¦ãªæƒ…å ±ã‚’æŠ½å‡º
        lines = context.split('\n')
        titles = [line for line in lines if line.startswith('ã‚¿ã‚¤ãƒˆãƒ«:')]
        contents = [line for line in lines if line.startswith('å†…å®¹:')]
        
        # Ubuntuç’°å¢ƒæ§‹ç¯‰ã«é–¢ã™ã‚‹ç‰¹åˆ¥å‡¦ç†
        if 'ubuntu' in question.lower() and ('ç’°å¢ƒæ§‹ç¯‰' in question or 'æ©Ÿæ¢°å­¦ç¿’' in question):
            answer = "Ubuntuç’°å¢ƒã§ã®æ©Ÿæ¢°å­¦ç¿’ç’°å¢ƒæ§‹ç¯‰ã«ã¤ã„ã¦ã€é–¢é€£ã™ã‚‹æƒ…å ±ã‚’ãŠä¼ãˆã—ã¾ã™ã€‚\n\n"
            
            # ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰é–¢é€£ã™ã‚‹æƒ…å ±ã‚’æŠ½å‡º
            relevant_info = []
            for title in titles:
                clean_title = title.replace('ã‚¿ã‚¤ãƒˆãƒ«:', '').strip()
                if any(keyword in clean_title for keyword in ['AI', 'æ©Ÿæ¢°å­¦ç¿’', 'Python', 'ã‚»ãƒŸãƒŠãƒ¼', 'ãƒ­ãƒœãƒƒãƒˆ']):
                    relevant_info.append(clean_title)
            
            if relevant_info:
                answer += "ã€é–¢é€£ã™ã‚‹å­¦ç¿’ãƒªã‚½ãƒ¼ã‚¹ã€‘\n"
                for info in relevant_info[:3]:
                    answer += f"â€¢ {info}\n"
                answer += "\n"
            
            answer += "ã€ä¸€èˆ¬çš„ãªç’°å¢ƒæ§‹ç¯‰æ‰‹é †ã€‘\n"
            answer += "1. Pythonç’°å¢ƒã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ— (Anaconda/Minicondaæ¨å¥¨)\n"
            answer += "2. å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« (numpy, pandas, scikit-learnç­‰)\n"
            answer += "3. GPUåˆ©ç”¨æ™‚ã¯CUDA/cuDNNã®è¨­å®š\n"
            answer += "4. Jupyter Notebookã®ç’°å¢ƒæ§‹ç¯‰\n\n"
            answer += "å·åˆç ”ç©¶å®¤ã§ã¯ã€AIãƒ»æ©Ÿæ¢°å­¦ç¿’åˆ†é‡ã®ç ”ç©¶ã‚’è¡Œã£ã¦ãŠã‚Šã€é–¢é€£ã™ã‚‹æŠ€è¡“æƒ…å ±ã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚"
            
            return answer
        
        # esaã§RAGã‚’ä½œã‚‹ã«é–¢ã™ã‚‹ç‰¹åˆ¥å‡¦ç†ï¼ˆã‚ˆã‚Šè©³ç´°åŒ–ï¼‰
        if 'esaã§RAGã‚’ä½œã‚‹' in ''.join(titles) or 'esa' in question.lower() and 'rag' in question.lower():
            answer = "esaã‹ã‚‰è¨˜äº‹ã‚’å–å¾—ã—ã¦RAGã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã™ã‚‹æ‰‹é †ã«ã¤ã„ã¦èª¬æ˜ã„ãŸã—ã¾ã™ã€‚\n\n"
            
            # å…·ä½“çš„ãªæ‰‹é †ã‚’æä¾›
            answer += "ã€ä¸»è¦ãªæ‰‹é †ã€‘\n"
            answer += "1. ESA APIã‚’ä½¿ç”¨ã—ãŸè¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã®å–å¾—\n"
            answer += "2. è¨˜äº‹å†…å®¹ã®å‰å‡¦ç†ã¨ãƒ™ã‚¯ãƒˆãƒ«åŒ–\n" 
            answer += "3. ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¸ã®æ ¼ç´\n"
            answer += "4. è³ªå•å¿œç­”ã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰\n\n"
            
            # è¨˜äº‹ã®å…·ä½“çš„å†…å®¹ãŒã‚ã‚Œã°è¿½åŠ 
            for content_line in contents:
                content = content_line.replace('å†…å®¹:', '').strip()
                if content:
                    # æŠ€è¡“çš„ãªè©³ç´°ã‚’æŠ½å‡º
                    if any(keyword in content for keyword in ['API', 'ã‚·ã‚¹ãƒ†ãƒ ', 'æ‰‹é †', 'æ–¹æ³•', 'ãƒ‡ãƒ¼ã‚¿']):
                        sentences = [s.strip() for s in content.split('ã€‚') if len(s.strip()) > 15][:2]
                        if sentences:
                            answer += "ã€æŠ€è¡“çš„è©³ç´°ã€‘\n"
                            answer += 'ã€‚'.join(sentences) + 'ã€‚\n\n'
                            break
            
            answer += "å·åˆç ”ç©¶å®¤ã§ã¯ã€ã“ã‚Œã‚‰ã®æŠ€è¡“ã‚’æ´»ç”¨ã—ã¦QAã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰ã‚’é€²ã‚ã¦ã„ã¾ã™ã€‚"
            
            return answer
        
        # ä¸€èˆ¬çš„ãªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†
        if titles:
            answer = "å·åˆç ”ç©¶å®¤ã®æœ€æ–°å‹•å‘ã«ã¤ã„ã¦ã€ä»¥ä¸‹ã®æƒ…å ±ã‚’ãŠä¼ãˆã—ã¾ã™ï¼š\n\n"
            
            # ã‚ˆã‚Šè‡ªç„¶ãªè¡¨ç¾ã§ã‚¿ã‚¤ãƒˆãƒ«ã¨æ¦‚è¦ã‚’çµ„ã¿åˆã‚ã›
            for i, title in enumerate(titles[:2]):  # æœ€å¤§2ä»¶ã«åˆ¶é™
                clean_title = title.replace('ã‚¿ã‚¤ãƒˆãƒ«:', '').strip()
                answer += f"â€¢ {clean_title}\n"
                
                # å¯¾å¿œã™ã‚‹å†…å®¹ãŒã‚ã‚Œã°è¦ç´„ã‚’è¿½åŠ 
                if i < len(contents):
                    content = contents[i].replace('å†…å®¹:', '').strip()
                    summary = self._create_natural_summary(content, max_length=80)
                    if summary:
                        answer += f"  {summary}\n"
            
            answer += "\nè©³ç´°ãªæƒ…å ±ã«ã¤ã„ã¦ã¯ã€å„è¨˜äº‹ã‚’ã”å‚ç…§ãã ã•ã„ã€‚"
        else:
            answer = "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚ç¾åœ¨ã‚·ã‚¹ãƒ†ãƒ ã®å‡¦ç†ã«å•é¡ŒãŒç™ºç”Ÿã—ã¦ãŠã‚Šã€è©³ç´°ãªå›ç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚\n"
            answer += "ã‚ˆã‚Šå…·ä½“çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§å†åº¦æ¤œç´¢ã—ã¦ã„ãŸã ã‘ã¾ã™ã§ã—ã‚‡ã†ã‹ã€‚"
        
        return answer
    
    def _create_natural_summary(self, content: str, max_length: int = 120) -> str:
        """è‡ªç„¶ãªè¦ç´„ã‚’ä½œæˆ"""
        if len(content) <= max_length:
            return content
        
        # æ–‡ã§åˆ†å‰²ã—ã¦é‡è¦ãªéƒ¨åˆ†ã‚’é¸æŠ
        sentences = [s.strip() + 'ã€‚' for s in content.split('ã€‚') if len(s.strip()) > 5]
        
        if not sentences:
            return content[:max_length] + "..."
        
        # æœ€åˆã®æ–‡ã‚’åŸºæœ¬ã¨ã™ã‚‹
        summary = sentences[0]
        
        # é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€æ–‡ã‚’æ¢ã—ã¦è¿½åŠ 
        important_keywords = ['ç ”ç©¶', 'é–‹ç™º', 'æŠ€è¡“', 'AI', 'æ©Ÿæ¢°å­¦ç¿’', 'æˆæœ', 'æ´»å‹•']
        for sentence in sentences[1:]:
            if len(summary) + len(sentence) > max_length:
                break
            if any(keyword in sentence for keyword in important_keywords):
                summary += sentence
                break
        
        if len(summary) > max_length:
            summary = summary[:max_length] + "..."
        
        return summary
    
    def _calculate_confidence(self, search_results: List) -> float:
        """ä¿¡é ¼åº¦è¨ˆç®—ï¼ˆZennã‚µã‚¤ãƒˆã®æ¤œè¨¼çµæœã‚’å‚è€ƒï¼‰"""
        if not search_results:
            return 0.0
        
        # æ¤œç´¢çµæœã®ã‚¹ã‚³ã‚¢ã¨è¨˜äº‹æ•°ã«åŸºã¥ãä¿¡é ¼åº¦
        scores = [result.score for result in search_results]
        avg_score = sum(scores) / len(scores)
        
        # è¨˜äº‹æ•°ã«ã‚ˆã‚‹è£œæ­£ï¼ˆã‚ˆã‚Šå¤šãã®è¨˜äº‹ãŒã‚ã‚‹ã¨ä¿¡é ¼åº¦å‘ä¸Šï¼‰
        article_count_bonus = min(len(search_results) / 10.0, 0.2)
        
        # æœ€é«˜ã‚¹ã‚³ã‚¢ãŒé«˜ã„å ´åˆã®è£œæ­£
        max_score = max(scores)
        high_score_bonus = 0.1 if max_score > 0.8 else 0.0
        
        confidence = min(avg_score + article_count_bonus + high_score_bonus, 1.0)
        
        return confidence
    
    def get_model_info(self) -> Dict[str, Any]:
        """ç¾åœ¨ä½¿ç”¨ä¸­ã®ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’è¿”ã™"""
        base_info = {
            "model_name": self.model_name,
            "model_type": "LangChain-style RAG",
            "capabilities": [
                "Document-based QA",
                "Context-aware responses", 
                "Multi-article synthesis"
            ],
            "implementation_based_on": "Zenn RAG tutorial"
        }
        
        # ãƒ‡ãƒã‚¤ã‚¹æƒ…å ±ã‚’è¿½åŠ 
        if hasattr(self, 'device_info'):
            base_info["device_config"] = self.device_info
            base_info["gpu_optimized"] = self.device_info["device_type"].startswith("GPU")
        
        return base_info
    
    def _log_device_info(self):
        """ãƒ‡ãƒã‚¤ã‚¹æƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›"""
        logger.info(f"PyTorch version: {torch.__version__}")
        logger.info(f"CUDA available: {torch.cuda.is_available()}")
        
        if torch.cuda.is_available():
            logger.info(f"CUDA device count: {torch.cuda.device_count()}")
            for i in range(torch.cuda.device_count()):
                device_name = torch.cuda.get_device_name(i)
                memory_total = torch.cuda.get_device_properties(i).total_memory / 1024**3
                logger.info(f"GPU {i}: {device_name} ({memory_total:.1f}GB)")
        else:
            logger.info("Running on CPU")
    
    def _get_optimal_device_config(self):
        """æœ€é©ãªãƒ‡ãƒã‚¤ã‚¹è¨­å®šã‚’è¿”ã™"""
        # è¨­å®šã«ã‚ˆã‚‹å¼·åˆ¶CPUä½¿ç”¨ãƒã‚§ãƒƒã‚¯
        from ..config.settings import settings
        if settings.force_cpu:
            logger.info("Forced to use CPU by configuration")
            return {
                "device_type": "CPU (Forced)",
                "device_map": None,
                "dtype": torch.float32,
                "pipeline_device": -1,
                "batch_size": 1,
                "memory_gb": 0
            }
        
        if torch.cuda.is_available() and settings.enable_gpu:
            # GPUåˆ©ç”¨å¯èƒ½ãªå ´åˆ
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            usable_memory = gpu_memory * settings.gpu_memory_fraction
            
            if usable_memory >= 10:  # 12GBä»¥ä¸Šã®GPU (RTX 3080 Tiä»¥ä¸Š)
                return {
                    "device_type": "GPU (High Memory)",
                    "device_map": "auto",
                    "dtype": torch.float16,
                    "pipeline_device": 0,
                    "batch_size": 8,
                    "memory_gb": gpu_memory,
                    "usable_memory_gb": usable_memory
                }
            elif usable_memory >= 5:  # 6GBä»¥ä¸Šã®GPU (RTX 3060ä»¥ä¸Š)
                return {
                    "device_type": "GPU (Medium Memory)", 
                    "device_map": "auto",
                    "dtype": torch.float16,
                    "pipeline_device": 0,
                    "batch_size": 4,
                    "memory_gb": gpu_memory,
                    "usable_memory_gb": usable_memory
                }
            else:  # 6GBæœªæº€ã®GPU
                return {
                    "device_type": "GPU (Low Memory)",
                    "device_map": "auto",
                    "dtype": torch.float16,
                    "pipeline_device": 0,
                    "batch_size": 2,
                    "memory_gb": gpu_memory,
                    "usable_memory_gb": usable_memory
                }
        else:
            # CPUåˆ©ç”¨ã®å ´åˆ
            return {
                "device_type": "CPU",
                "device_map": None,
                "dtype": torch.float32,
                "pipeline_device": -1,
                "batch_size": 1,
                "memory_gb": 0
            }

    def answer_question_with_context(self, question: str, contexts: List, progress_tracker=None, **kwargs) -> QAResult:
        """
        ä¸ãˆã‚‰ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½¿ç”¨ã—ã¦è³ªå•ã«ç­”ãˆã‚‹
        ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢çµæœã‚’å—ã‘å–ã£ã¦å›ç­”ã‚’ç”Ÿæˆ
        """
        try:
            logger.info(f"Processing question with provided context: {question[:50]}...")
            logger.info(f"Number of contexts provided: {len(contexts)}")
            
            # é€²æ—æ›´æ–°: é–‹å§‹
            if progress_tracker:
                progress_tracker.update(10, "è³ªå•ã®è§£æã‚’é–‹å§‹ã—ã¦ã„ã¾ã™...", {"question": question[:50]})
            
            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã®Articleå½¢å¼å¤‰æ›
            articles = []
            for ctx in contexts:
                # HybridSearchResultã‹ã‚‰Articleã¸ã®å¤‰æ›
                if hasattr(ctx, 'article_id') and hasattr(ctx, 'title'):
                    # ç°¡æ˜“Articleã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä½œæˆ
                    from ..models.esa_models import Article
                    
                    article = Article(
                        number=ctx.article_id,
                        name=ctx.title,
                        full_name=ctx.title,
                        wip=False,
                        body_md=ctx.content if hasattr(ctx, 'content') else '',
                        body_html='',
                        created_at=datetime.now(),
                        updated_at=datetime.now(),
                        tags=[],
                        category='',
                        url='',
                        created_by_id=0,
                        updated_by_id=0,
                        processed_text=ctx.content if hasattr(ctx, 'content') else ''
                    )
                    articles.append(article)
                elif hasattr(ctx, 'article'):
                    # æ—¢ã«Articleå½¢å¼ã®å ´åˆ
                    articles.append(ctx.article)
            
            # é€²æ—æ›´æ–°: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†
            if progress_tracker:
                progress_tracker.update(25, f"ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å‡¦ç†ä¸­... ({len(articles)}ä»¶ã®è¨˜äº‹)", 
                                      {"articles_count": len(articles)})
            
            if not articles:
                logger.warning("No valid articles found in provided contexts")
                if progress_tracker:
                    progress_tracker.error("æœ‰åŠ¹ãªè¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return QAResult(
                    question=question,
                    answer="ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€æä¾›ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å›ç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚",
                    source_articles=[],
                    confidence=0.0,
                    generated_at=datetime.now()
                )
            
            # é€²æ—æ›´æ–°: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰
            if progress_tracker:
                progress_tracker.update(40, "ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹ç¯‰ä¸­...", 
                                      {"processing_articles": len(articles)})
            
            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ–‡å­—åˆ—ã‚’æ§‹ç¯‰
            context_parts = []
            sources = []
            
            for i, article in enumerate(articles[:kwargs.get('context_limit', 5)], 1):
                context_part = f"ã€è¨˜äº‹{i}: {article.name}ã€‘\n"
                
                if article.category:
                    context_part += f"åˆ†é‡: {article.category}\n"
                
                if article.tags:
                    relevant_tags = [tag for tag in article.tags if tag.strip()][:3]
                    if relevant_tags:
                        context_part += f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(relevant_tags)}\n"
                
                # è¨˜äº‹å†…å®¹ã‚’è¿½åŠ 
                content = ""
                if hasattr(article, 'processed_text') and article.processed_text:
                    content = self._extract_key_sentences(article.processed_text, 3)
                elif article.body_md:
                    clean_text = self._clean_markdown(article.body_md)
                    content = self._extract_key_sentences(clean_text, 3)
                
                if content:
                    context_part += f"å†…å®¹: {content}\n"
                
                context_parts.append(context_part)
                
                # ã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’è¿½åŠ 
                sources.append({
                    "article_number": article.number,
                    "title": article.name,
                    "category": article.category or "æœªåˆ†é¡",
                    "url": article.url or ""
                })
            
            combined_context = "\n".join(context_parts)
            logger.info(f"ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·: {len(combined_context)}æ–‡å­—")
            
            # é€²æ—æ›´æ–°: è³ªå•åˆ†æ
            if progress_tracker:
                progress_tracker.update(60, "è³ªå•ã‚¿ã‚¤ãƒ—ã‚’åˆ†æä¸­...", 
                                      {"context_length": len(combined_context)})
            
            # è³ªå•ã‚¿ã‚¤ãƒ—ã‚’åˆ†æ
            question_type = self._analyze_question_type(question)
            
            # é€²æ—æ›´æ–°: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
            if progress_tracker:
                progress_tracker.update(70, "AIãƒ¢ãƒ‡ãƒ«ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰ä¸­...", 
                                      {"question_type": question_type})
            
            # LLMãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
            prompt = self._create_conversational_prompt(question, combined_context, question_type)
            logger.info(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·: {len(prompt)}æ–‡å­—")
            
            # é€²æ—æ›´æ–°: LLMæ¨è«–é–‹å§‹
            if progress_tracker:
                progress_tracker.update(80, "AIãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹å›ç­”ç”Ÿæˆä¸­...", 
                                      {"prompt_length": len(prompt)})
            
            # LLMæ¨è«–å®Ÿè¡Œ
            response_text = self._generate_answer_with_prompt(question, combined_context)
            
            # é€²æ—æ›´æ–°: å“è³ªãƒã‚§ãƒƒã‚¯
            if progress_tracker:
                progress_tracker.update(95, "å›ç­”ã®å“è³ªã‚’ãƒã‚§ãƒƒã‚¯ä¸­...", 
                                      {"response_length": len(response_text)})
            
            # å“è³ªãƒã‚§ãƒƒã‚¯ - ç°¡æ˜“çš„ãªä¿¡é ¼åº¦è¨ˆç®—
            confidence = 0.8 if len(articles) >= 3 else 0.6
            
            result = QAResult(
                question=question,
                answer=response_text,
                source_articles=articles[:kwargs.get('context_limit', 5)],
                confidence=confidence,
                generated_at=datetime.now()
            )
            
            # é€²æ—æ›´æ–°: å®Œäº†
            if progress_tracker:
                progress_tracker.complete("å›ç­”ç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸ", {
                    "confidence": confidence,
                    "articles_used": len(articles),
                    "response_length": len(response_text)
                })
            
            logger.info(f"å›ç­”ç”Ÿæˆå®Œäº† - ä¿¡é ¼åº¦: {confidence:.3f}")
            return result
            
        except Exception as e:
            logger.error(f"QAå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            
            # é€²æ—æ›´æ–°: ã‚¨ãƒ©ãƒ¼
            if progress_tracker:
                progress_tracker.error(str(e))
                
            return QAResult(
                question=question,
                answer=f"ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€å›ç­”ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}",
                source_articles=[],
                confidence=0.0,
                generated_at=datetime.now()
            )
